{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d1ee5a-ac99-49fd-a423-73d69a16d0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/data/exploratory_notebooks/da/src/hear21passt\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: timm==0.4.12 in /opt/conda/lib/python3.10/site-packages (from hear21passt==0.0.23) (0.4.12)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from hear21passt==0.0.23) (0.13.1)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from timm==0.4.12->hear21passt==0.0.23) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.4.12->hear21passt==0.0.23) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4->timm==0.4.12->hear21passt==0.0.23) (0.38.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12->hear21passt==0.0.23) (1.23.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12->hear21passt==0.0.23) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.4.12->hear21passt==0.0.23) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12->hear21passt==0.0.23) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12->hear21passt==0.0.23) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12->hear21passt==0.0.23) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->timm==0.4.12->hear21passt==0.0.23) (2023.5.7)\n",
      "Installing collected packages: hear21passt\n",
      "  Attempting uninstall: hear21passt\n",
      "    Found existing installation: hear21passt 0.0.23\n",
      "    Uninstalling hear21passt-0.0.23:\n",
      "      Successfully uninstalled hear21passt-0.0.23\n",
      "  Running setup.py develop for hear21passt\n",
      "Successfully installed hear21passt-0.0.23\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (0.25.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e 'src/hear21passt'\n",
    "%pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9491ffdf-8060-445c-9481-6a076e622dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.56.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.28.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (65.6.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.26.14)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7236e-0cff-4355-974f-a8cc9982466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6b71e1-831e-407d-aa22-b793dc5d943e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import tqdm\n",
    "\n",
    "from operator import itemgetter\n",
    "from hashlib import sha1\n",
    "\n",
    "from torch.optim import AdamW, lr_scheduler as LR\n",
    "from torch.utils.data import default_collate\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import ConcatDataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from optim.lr_scheduler import get_scheduler_lambda, ReduceLROnPlateauHumanized\n",
    "from utils.model import get_pretrained_passt_model\n",
    "from utils.gdsc22_dataset_legacy import BuzzMapDataset, BuzzDatasetWeighter, BuzzMapTestDataset\n",
    "from utils.dataset_augmentations import (\n",
    "    BuzzMapTransformedDataset, \n",
    "    BuzzMapTransformedTestDataset,\n",
    "    BuzzTorchDataLoader,\n",
    "    MixUp1Level,\n",
    "    MixUp2Level, \n",
    "    RandomGain,\n",
    "    Rolling,\n",
    "    ColoredNoise,\n",
    "    NormalizeAmplitudes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1d9ff-318e-4dc0-a2f5-01b771f2bf39",
   "metadata": {},
   "source": [
    "## 1. Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4199738a-787c-4f87-8545-cee4de644e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d618b58c-2694-474f-be30-2b10b18a43f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "data_processed_dir = os.path.join(project_dir, 'gdsc_data/data_processed/')\n",
    "data_source_dir = os.path.join(project_dir, 'gdsc_data/data_source/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a8cef4-c81e-472d-8787-6bb9321f40ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "MODEL_SAMPLING_RATE = 32000\n",
    "\n",
    "MODEL_NAME = f\"gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_long_short_samples_resampled_epochs10_transformation=[normed]@{ts}.pt\"\n",
    "TORCH_SAVE_PATH = os.path.join(project_dir, f'checkpoints/{MODEL_NAME}')\n",
    "\n",
    "TENSORBOARD_RUNS_PATH = os.path.join(project_dir, \"torch_runlogs/\")\n",
    "TENSORBOARD_EXP_FOLDER = f\"gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_long_short_samples_resampled_epochs10_transformation=[normed]@{ts}\"\n",
    "TENSORBOARD_EXP_COMMENT = \"PaSST_s_kd_p16_128_ap486_NOEXP_lre_mean_loss_train+val_stratified_long_short_samples_resampled_epochs10_transformation=[normed]\"\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "# fine tuning and ohe of real labels\n",
    "CLASSIFIER_N_CLASSES = 66\n",
    "\n",
    "DATA_TAG = \"data_8c86715\"\n",
    "\n",
    "DEBUG_TRANSFORMATIONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e629ab-d31f-4a03-8852-c711c6e71fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#final_dataset_path = find_final_dataset_path(data_processed_dir, DATA_TAG)\n",
    "final_dataset_path_augmented = os.path.join(data_processed_dir, DATA_TAG, '01_apply_ir_function')\n",
    "final_dataset_val_as_train_path_augmented = os.path.join(data_processed_dir, DATA_TAG, '03_apply_ir_function_on_validation')\n",
    "\n",
    "final_dataset_path_short = os.path.join(data_processed_dir, DATA_TAG, '07_split_original_into_windows_with_short_samples/')\n",
    "final_dataset_path_long = os.path.join(data_processed_dir, DATA_TAG, '07_split_original_into_windows_with_short_samples/')\n",
    "\n",
    "final_dataset_val_as_train_path_short = os.path.join(data_processed_dir, DATA_TAG, '08_spit_resampled_validation_into_windows_with_short_samples/')\n",
    "final_dataset_val_as_train_path_long = os.path.join(data_processed_dir, DATA_TAG, '08_spit_resampled_validation_into_windows_with_short_samples')\n",
    "\n",
    "final_dataset_val_as_train_path = os.path.join(data_processed_dir, DATA_TAG, '05_split_resampled_validation_into_windows/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b780f547-b2e3-4e48-a2a8-acaf7b4f601c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir_augmented = os.path.join(final_dataset_path_augmented, 'train/')\n",
    "val_as_train_dir_augmented = os.path.join(final_dataset_val_as_train_path_augmented, 'train/')\n",
    "\n",
    "train_dir_short = os.path.join(final_dataset_path_short, 'train/short_only')\n",
    "train_dir_long = os.path.join(final_dataset_path_short, 'train/long_only')\n",
    "val_as_train_dir_short = os.path.join(final_dataset_val_as_train_path_short, 'train/short_only')\n",
    "val_as_train_dir_long = os.path.join(final_dataset_val_as_train_path_long, 'train/long_only')\n",
    "new_sampled_val_dir = os.path.join(final_dataset_val_as_train_path, 'val/')\n",
    "\n",
    "val_dir = os.path.join(final_dataset_val_as_train_path, 'val/')\n",
    "#test_dir = os.path.join(final_dataset_test_path, 'test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd4b84-1417-4f51-ba22-fc23fe796da4",
   "metadata": {},
   "source": [
    "## 2. Dataloaders and batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78b39c-e346-4d81-a19e-e325ebf2fad6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Debugging collate_fn\n",
    "\n",
    "Used to check the random seed and batchability of the individual points 0:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef39925d-533a-4c4a-b1f1-55bb8017c771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def debug_collate(batch):\n",
    "    get_1st = itemgetter(0)\n",
    "    files = \", \".join(map(get_1st, batch)).encode('utf-8')\n",
    "    worker_info = torch.utils.data.get_worker_info()   \n",
    "    print(f\"Worker_id '{worker_info.id}' batch hash is '{sha1(files).hexdigest()}'\")\n",
    "    \n",
    "    batch = default_collate(batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37654458-8c76-4cce-a655-df8d89b42579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds_short = BuzzMapDataset(\n",
    "    train_dir_short,\n",
    "    metadata_csv=os.path.join(train_dir_short, 'metadata.csv'), \n",
    "    num_classes=CLASSIFIER_N_CLASSES,\n",
    ")\n",
    "\n",
    "train_ds_long = BuzzMapDataset(\n",
    "    train_dir_long,\n",
    "    metadata_csv=os.path.join(train_dir_long, 'metadata.csv'), \n",
    "    num_classes=CLASSIFIER_N_CLASSES,\n",
    ")\n",
    "\n",
    "val_ds_short = BuzzMapDataset(\n",
    "    val_as_train_dir_short,\n",
    "    metadata_csv=os.path.join(val_as_train_dir_short, 'metadata.csv'), \n",
    "    num_classes=CLASSIFIER_N_CLASSES\n",
    ")\n",
    "\n",
    "val_ds_long = BuzzMapDataset(\n",
    "    val_as_train_dir_long,\n",
    "    metadata_csv=os.path.join(val_as_train_dir_long, 'metadata.csv'), \n",
    "    num_classes=CLASSIFIER_N_CLASSES\n",
    ")\n",
    "\n",
    "new_val_dataset = BuzzMapDataset(\n",
    "    new_sampled_val_dir,\n",
    "    metadata_csv=os.path.join(new_sampled_val_dir, 'metadata.csv'), \n",
    "    num_classes=CLASSIFIER_N_CLASSES    \n",
    ")\n",
    "\n",
    "# test_dataset = BuzzMapTestDataset(\n",
    "#     test_dir,\n",
    "#     metadata_csv=os.path.join(val_as_train_dir, 'metadata.csv'),\n",
    "#     num_classes=CLASSIFIER_N_CLASSES\n",
    "# )\n",
    "\n",
    "train_dataset_short = BuzzMapTransformedDataset(\n",
    "    transformers=[\n",
    "        NormalizeAmplitudes(debug=DEBUG_TRANSFORMATIONS), \n",
    "        #RandomGain(debug=DEBUG_TRANSFORMATIONS), \n",
    "        #Rolling(shift_sec=0.5, SR=MODEL_SAMPLING_RATE, debug=DEBUG_TRANSFORMATIONS), \n",
    "        #MixUp1Level(debug=DEBUG_TRANSFORMATIONS), \n",
    "        #ColoredNoise(debug=DEBUG_TRANSFORMATIONS)\n",
    "    ],\n",
    "    buzz_mappable=train_ds_short,\n",
    "    #debug=True\n",
    ")\n",
    "\n",
    "train_dataset_long = BuzzMapTransformedDataset(\n",
    "    transformers=[\n",
    "        NormalizeAmplitudes(debug=DEBUG_TRANSFORMATIONS)\n",
    "    ],\n",
    "    buzz_mappable=train_ds_long,\n",
    "    #debug=True\n",
    ")\n",
    "\n",
    "val_dataset_short = BuzzMapTransformedDataset(\n",
    "    transformers=[\n",
    "        NormalizeAmplitudes(debug=DEBUG_TRANSFORMATIONS)\n",
    "    ],\n",
    "    buzz_mappable=val_ds_short,\n",
    "    #debug=True\n",
    ")\n",
    "\n",
    "val_dataset_long = BuzzMapTransformedDataset(\n",
    "    transformers=[\n",
    "        NormalizeAmplitudes(debug=DEBUG_TRANSFORMATIONS), \n",
    "        #RandomGain(debug=DEBUG_TRANSFORMATIONS), \n",
    "        #Rolling(shift_sec=0.5, SR=MODEL_SAMPLING_RATE, debug=DEBUG_TRANSFORMATIONS), \n",
    "        #MixUp1Level(debug=DEBUG_TRANSFORMATIONS), \n",
    "        #ColoredNoise(debug=DEBUG_TRANSFORMATIONS)\n",
    "    ],\n",
    "    buzz_mappable=val_ds_long,\n",
    "    #debug=True\n",
    ")\n",
    "\n",
    "new_val_dataset_normed = BuzzMapTransformedDataset(\n",
    "    transformers=[\n",
    "        NormalizeAmplitudes(debug=DEBUG_TRANSFORMATIONS)\n",
    "    ],\n",
    "    buzz_mappable=new_val_dataset,\n",
    "    #debug=True\n",
    ")\n",
    "\n",
    "# test_dataset_normed = BuzzMapTransformedTestDataset(\n",
    "#     transformers=[\n",
    "#         NormalizeAmplitudes(debug=DEBUG_TRANSFORMATIONS)\n",
    "#     ],\n",
    "#     buzz_mappable=test_dataset,\n",
    "#     #debug=True    \n",
    "# )\n",
    "\n",
    "# I had too much hope on the IR augmented dataset and took a wrong path...\n",
    "# train_dataset_augmented = BuzzAugmentedIterableDataset(\n",
    "#     train_dir_augmented, \n",
    "#     metadata_csv=os.path.join(final_dataset_path_augmented, 'metadata_grouped_train.csv'), \n",
    "#     shuffle=True,\n",
    "#     subsample=0.2,\n",
    "#     total_classes=CLASSIFIER_N_CLASSES\n",
    "# )\n",
    "# val_as_train_dataset_augmented = BuzzAugmentedIterableDataset(\n",
    "#     val_as_train_dir_augmented, \n",
    "#     metadata_csv=os.path.join(final_dataset_val_as_train_path_augmented, 'metadata_grouped_train.csv'), \n",
    "#     shuffle=True,\n",
    "#     subsample=0.2,\n",
    "#     total_classes=CLASSIFIER_N_CLASSES\n",
    "# )\n",
    "\n",
    "dataset_weights_train_val, num_samples = BuzzDatasetWeighter()(train_ds_short, train_ds_long, val_ds_short, val_ds_long)\n",
    "\n",
    "# train_dataloader_normed = BuzzTorchDataLoader(\n",
    "#     train_dataset_normed, \n",
    "#     batch_size=TRAIN_BATCH_SIZE, \n",
    "#     pin_memory=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     prefetch_factor=2,\n",
    "#     worker_init_fn=train_dataset_normed._init_multiprocess_worker,    \n",
    "# )\n",
    "    \n",
    "train_dataloader = BuzzTorchDataLoader(\n",
    "    ConcatDataset([train_ds_short, train_ds_long, val_ds_short, val_ds_long]),\n",
    "    batch_size=TRAIN_BATCH_SIZE, \n",
    "    pin_memory=True,\n",
    "    sampler=WeightedRandomSampler(dataset_weights_train_val, num_samples),\n",
    "    num_workers=3,\n",
    "    prefetch_factor=2,\n",
    "    worker_init_fn=new_val_dataset_normed._init_multiprocess_worker,\n",
    "    #collate_fn=debug_collate\n",
    ")\n",
    "\n",
    "# val_dataloader = BuzzTorchDataLoader(\n",
    "#     val_dataset_normed, \n",
    "#     batch_size=VAL_BATCH_SIZE, \n",
    "#     pin_memory=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2,\n",
    "#     prefetch_factor=2,\n",
    "#     worker_init_fn=train_dataset_normed._init_multiprocess_worker,        \n",
    "# )\n",
    "\n",
    "new_val_dataloader = BuzzTorchDataLoader(\n",
    "    new_val_dataset_normed, \n",
    "    batch_size=VAL_BATCH_SIZE, \n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    prefetch_factor=2,\n",
    "    worker_init_fn=new_val_dataset_normed._init_multiprocess_worker,            \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15af16a-4065-4967-bc02-e967a18b273e",
   "metadata": {},
   "source": [
    "## 3. Init pytorch objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d1dc6c-1d1b-43ae-b2dd-8a100a72ab75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(os.path.join(TENSORBOARD_RUNS_PATH, TENSORBOARD_EXP_FOLDER), comment=TENSORBOARD_EXP_COMMENT)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d44a4b8-b0e9-40b6-8ae7-887effefeae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 16 19:50:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   45C    P0    27W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf3576b-0b56-4716-bb63-384994a57447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PaSST pre-trained on AudioSet (with KD) Patch 16 stride 10 structured patchout mAP=486 \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=66, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=66, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaSSTGDSCIface(\n",
       "  (mel): AugmentMelSTFT(\n",
       "    winsize=800, hopsize=320\n",
       "    (freqm): FrequencyMasking()\n",
       "    (timem): TimeMasking()\n",
       "  )\n",
       "  (net): PaSST(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pre_logits): Identity()\n",
       "    (head): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=66, bias=True)\n",
       "    )\n",
       "    (head_dist): Linear(in_features=768, out_features=66, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default stride for t and f is 10\n",
    "# NOTE: FMAX is used in frequency augmentation. Every sample has a random subset of fmin and fmax applied to it...\n",
    "model = get_pretrained_passt_model(mode=\"all\", n_classes=66, s_patchout_t=40, s_patchout_f=4)\n",
    "\n",
    "#model = get_pretrained_passt_model(arch='passt_s_swa_p16_128_ap476', mode=\"all\", n_classes=66, s_patchout_t=10, s_patchout_f=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7122829-2b90-4d50-af9c-ebe39130c767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP1: 20epochs using <torch.utils.data.dataset.ConcatDataset object at 0x7f61b67f55a0>\n",
      "\n",
      "Val using <class 'utils.dataset_augmentations.BuzzMapTransformedDataset'>: Transformations=(<utils.dataset_augmentations.NormalizeAmplitudes object at 0x7f61b67f7f70>); Mapper=(<class 'utils.gdsc22_dataset_legacy.BuzzMapDataset'>: reading data from /root/data/gdsc_data/data_processed/data_8c86715/05_split_resampled_validation_into_windows/val/metadata.csv. Dataset stats: shape=(358, 3))\n",
      "\n",
      "Initial Learning rate:[1e-05]\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/717 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:632: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([32, 1, 128, 1000])\n",
      "self.norm(x) torch.Size([32, 768, 12, 99])\n",
      " patch_embed :  torch.Size([32, 768, 12, 99])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 99])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 12, 1])\n",
      "X Before time Patchout of 40  torch.Size([32, 768, 12, 99])\n",
      "X after time Patchout torch.Size([32, 768, 12, 59])\n",
      "X Before Freq Patchout of 4  torch.Size([32, 768, 12, 59])\n",
      " \n",
      " X after freq Patchout:  torch.Size([32, 768, 8, 59])\n",
      "X flattened torch.Size([32, 472, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 2, 768])\n",
      " self.cls_tokens.shape torch.Size([32, 1, 768])\n",
      " self.dist_token.shape torch.Size([32, 1, 768])\n",
      " final sequence x torch.Size([32, 474, 768])\n",
      " after 12 atten blocks x torch.Size([32, 474, 768])\n",
      "forward_features torch.Size([32, 768])\n",
      "head torch.Size([32, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TrainL 1.96:  84%|████████▎ | 600/717 [09:29<01:51,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "adamw = AdamW(model.net.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.LambdaLR(adamw, get_scheduler_lambda(warm_up_len=0))\n",
    "lr_scheduler = LR.StepLR(adamw, step_size=10, gamma=0.5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "TORCH_SAVE_FILE, EXT = os.path.splitext(TORCH_SAVE_PATH)\n",
    "\n",
    "train_loop_dataloader = train_dataloader\n",
    "val_loop_dataloader = new_val_dataloader\n",
    "\n",
    "epoch_begin = 0\n",
    "epoch_count = 20\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"STEP1: {epoch_count}epochs using {train_loop_dataloader}\\r\\n\")\n",
    "print(f\"Val using {val_loop_dataloader}\\r\\n\")\n",
    "print(f\"Initial Learning rate:{lr_scheduler.get_last_lr()}\\r\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "LOSS_REDUCTION_METHOD = 'mean'\n",
    "\n",
    "\n",
    "for epoch in range(epoch_begin, epoch_count):\n",
    "    y_tr_pred_f1 = []\n",
    "    y_tr_true_f1 = []\n",
    "    train_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for _, _, X_batch, y in (progress := tqdm.tqdm(train_loop_dataloader)):\n",
    "        adamw.zero_grad()        \n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y = y.to(device)\n",
    "        X_batch_spectr = model.mel(X_batch).unsqueeze(1)\n",
    "        \n",
    "        with  torch.cuda.amp.autocast(): \n",
    "            y_hat, emb = model.net(X_batch_spectr)     \n",
    "            \n",
    "            loss = F.cross_entropy(y_hat, y, reduction=\"none\")\n",
    "            loss_reduce_method = getattr(loss, LOSS_REDUCTION_METHOD)\n",
    "            loss = loss_reduce_method()            \n",
    "            \n",
    "            train_loss.append(loss.detach().cpu())\n",
    "            progress.set_description(f\"TrainL {float(loss):.2f}\")\n",
    "            \n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(adamw)\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        \n",
    "        prob = F.softmax(y_hat.detach(), dim=1)\n",
    "        y_pred = torch.argmax(prob.detach(), dim=1)\n",
    "        y_tr_pred_f1.append(y_pred.cpu())\n",
    "        y_tr_true_f1.append(y.cpu().argmax(dim=1))\n",
    "\n",
    "    f_score_train = f1_score(torch.cat(y_tr_true_f1).numpy(), torch.cat(y_tr_pred_f1).numpy(), average='macro')\n",
    "    total_train_loss = torch.stack(train_loss, dim=0)\n",
    "    total_train_reduction_method = getattr(total_train_loss, LOSS_REDUCTION_METHOD)\n",
    "    total_train_loss = total_train_reduction_method()\n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "    print(f\"Train Loss(epoch={epoch}): {total_train_loss}\")\n",
    "    print(f\"F1 score(train) {f_score_train}\")\n",
    "    print(\"\\r\\n\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "\n",
    "    val_loss = []\n",
    "    y_val_pred_f1 = []\n",
    "    y_val_true_f1 = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, _, _, X_batch, y_val in (progress := tqdm.tqdm(val_loop_dataloader)):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            X_batch_spectr = model.mel(X_batch).unsqueeze(1)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_hat, emb = model.net(X_batch_spectr)\n",
    "            \n",
    "                _loss = F.cross_entropy(y_hat, y_val, reduction=\"none\")\n",
    "                _loss_reduce_method = getattr(_loss, LOSS_REDUCTION_METHOD)\n",
    "                _loss = _loss_reduce_method()\n",
    "                val_loss.append(_loss.detach().cpu())\n",
    "            \n",
    "            progress.set_description(f\"ValL {float(_loss):.2f}\")            \n",
    "\n",
    "            prob = F.softmax(y_hat.detach(), dim=1)\n",
    "            y_pred = torch.argmax(prob.detach(), dim=1)\n",
    "\n",
    "            y_val_pred_f1.append(y_pred.cpu())\n",
    "            y_val_true_f1.append(y_val.cpu().argmax(dim=1))\n",
    "            \n",
    "    f1_score_val = f1_score(torch.cat(y_val_true_f1).numpy(), torch.cat(y_val_pred_f1).numpy(), average='macro')\n",
    "    total_val_loss = torch.stack(val_loss, dim=0)\n",
    "    total_val_loss_reduction_method = getattr(total_val_loss, LOSS_REDUCTION_METHOD)\n",
    "    total_val_loss = total_val_loss_reduction_method()\n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "    print(f\"Val Loss: {total_val_loss}\")\n",
    "    print(f\"F1 score(val) {f1_score_val}\")\n",
    "    print(\"\\r\\n\")    \n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "          \n",
    "    writer.add_scalar('Loss/train', total_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', total_val_loss, epoch)\n",
    "    writer.add_scalar('F1/train', f_score_train, epoch)\n",
    "    writer.add_scalar('F1/val', f1_score_val, epoch)\n",
    "    writer.add_scalar('LR', lr_scheduler.get_last_lr()[0], epoch)\n",
    "    \n",
    "    lr_scheduler.step()        \n",
    "\n",
    "    print(f\"New lr: {lr_scheduler.get_last_lr()[0]}\")\n",
    "       \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': adamw.state_dict(),\n",
    "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'grad_scaler_state_dict': scaler.state_dict(),\n",
    "        }, f\"{TORCH_SAVE_FILE}_epoch={epoch}_epochs=[{epoch_begin}, {epoch_begin+epoch_count}]{EXT}\")\n",
    "    \n",
    "    \n",
    "# print(f\"STEP2: 20epochs on scaled + 'transformed' data\")    \n",
    "# for epoch in range(20, 40):\n",
    "#     y_tr_pred_f1 = []\n",
    "#     y_tr_true_f1 = []\n",
    "#     train_loss = []\n",
    "\n",
    "#     model.train()\n",
    "#     for _, _, _, X_batch, y in (progress := tqdm.tqdm(train_dataloader_transformed)):\n",
    "#         adamw.zero_grad()        \n",
    "        \n",
    "#         X_batch = X_batch.to(device)\n",
    "#         y = y.to(device)\n",
    "#         X_batch_spectr = model.mel(X_batch).unsqueeze(1)\n",
    "        \n",
    "#         with  torch.cuda.amp.autocast(): \n",
    "#             y_hat, emb = model.net(X_batch_spectr)     \n",
    "            \n",
    "#             loss = F.cross_entropy(y_hat, y, reduction=\"none\")\n",
    "#             loss = loss.mean()\n",
    "            \n",
    "#             train_loss.append(loss.detach().cpu())\n",
    "#             progress.set_description(f\"TrainL {float(loss):.2f}\")\n",
    "            \n",
    "#         # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "#         # Backward passes under autocast are not recommended.\n",
    "#         # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "#         scaler.scale(loss).backward()\n",
    "#         # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "#         # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "#         # otherwise, optimizer.step() is skipped.\n",
    "#         scaler.step(adamw)\n",
    "#         # Updates the scale for next iteration.\n",
    "#         scaler.update()\n",
    "        \n",
    "#         prob = F.softmax(y_hat.detach(), dim=1)\n",
    "#         y_pred = torch.argmax(prob.detach(), dim=1)\n",
    "#         y_tr_pred_f1.append(y_pred.cpu())\n",
    "#         y_tr_true_f1.append(y.cpu().argmax(dim=1))\n",
    "\n",
    "#     f_score_train = f1_score(torch.cat(y_tr_true_f1).numpy(), torch.cat(y_tr_pred_f1).numpy(), average='macro')\n",
    "#     total_train_loss = torch.stack(train_loss, dim=0).mean()\n",
    "#     print(f\"=\" * 80)\n",
    "#     print(\"\\r\\n\")\n",
    "#     print(f\"Train Loss(epoch={epoch}): {total_train_loss}\")\n",
    "#     print(f\"F1 score(train) {f_score_train}\")\n",
    "#     print(\"\\r\\n\")\n",
    "#     print(f\"=\" * 80)\n",
    "#     print(\"\\r\\n\")\n",
    "\n",
    "#     val_loss = []\n",
    "#     y_val_pred_f1 = []\n",
    "#     y_val_true_f1 = []\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for _, _, _, X_batch, y_val in (progress := tqdm.tqdm(val_dataloader_normed)):\n",
    "#             X_batch = X_batch.to(device)\n",
    "#             y_val = y_val.to(device)\n",
    "#             X_batch_spectr = model.mel(X_batch).unsqueeze(1)\n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 y_hat, emb = model.net(X_batch_spectr)\n",
    "            \n",
    "#                 _loss = F.cross_entropy(y_hat, y_val, reduction=\"none\")\n",
    "#                 _loss = _loss.mean()\n",
    "#                 val_loss.append(_loss.detach().cpu())\n",
    "            \n",
    "#             progress.set_description(f\"ValL {float(_loss):.2f}\")            \n",
    "\n",
    "#             prob = F.softmax(y_hat.detach(), dim=1)\n",
    "#             y_pred = torch.argmax(prob.detach(), dim=1)\n",
    "\n",
    "#             y_val_pred_f1.append(y_pred.cpu())\n",
    "#             y_val_true_f1.append(y_val.cpu().argmax(dim=1))\n",
    "            \n",
    "#     f1_score_val = f1_score(torch.cat(y_val_true_f1).numpy(), torch.cat(y_val_pred_f1).numpy(), average='macro')\n",
    "#     total_val_loss = torch.stack(val_loss, dim=0).mean()\n",
    "#     print(f\"=\" * 80)\n",
    "#     print(\"\\r\\n\")\n",
    "#     print(f\"Val Loss: {total_val_loss}\")\n",
    "#     print(f\"F1 score(val) {f1_score_val}\")\n",
    "#     print(\"\\r\\n\")    \n",
    "#     print(f\"=\" * 80)\n",
    "#     print(\"\\r\\n\")\n",
    "       \n",
    "#     writer.add_scalar('Loss/train', total_train_loss, epoch)\n",
    "#     writer.add_scalar('Loss/val', total_val_loss, epoch)\n",
    "#     writer.add_scalar('F1/train', f_score_train, epoch)\n",
    "#     writer.add_scalar('F1/val', f1_score_val, epoch)\n",
    "#     writer.add_scalar('LR', lr_scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "#     lr_scheduler.step()\n",
    "#     print(f\"New lr: {lr_scheduler.get_last_lr()[0]}\")\n",
    "       \n",
    "#     torch.save(\n",
    "#         {\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': adamw.state_dict(),\n",
    "#             'grad_scaler_state_dict': scaler.state_dict(),\n",
    "#         }, f\"{TORCH_SAVE_FILE}_2nd20epochs(transformed_dataset){EXT}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c512d79f-f081-4abf-a347-ac5a2c6b50ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 82G\n",
      "drwxr-xr-x  2 root root     22K Jul 16 18:20  .\n",
      "drwx------ 21 root nogroup 6.0K Jul 16 13:52  ..\n",
      "-rw-r--r--  1 root root    978M Jul 16 16:54 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=0_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 17:03 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=1_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 17:13 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=2_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 17:22 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=3_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 17:32 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=4_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 17:42 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=5_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 17:51 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=6_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 18:01 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=7_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 18:10 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=8_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 18:20 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t10_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T16:44:23_epoch=9_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 12 16:30  gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_epochs40_gain_roll_mixup_noise@2023-07-12T16:26:10.pt\n",
      "-rw-r--r--  1 root root    978M Jul 12 17:16  gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_epochs40_gain_roll_mixup_noise@2023-07-12T17:01:22.pt\n",
      "-rw-r--r--  1 root root    978M Jul 12 17:26  gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_epochs40_gain_roll_mixup_noise@2023-07-12T17:16:51.pt\n",
      "-rw-r--r--  1 root root    978M Jul 12 22:09  gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_epochs40_gain_roll_mixup_noise@2023-07-12T17:28:35.pt\n",
      "-rw-r--r--  1 root root    978M Jul 13 03:43  gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_epochs40_gain_roll_mixup_noise@2023-07-12T22:47:03.pt\n",
      "-rw-r--r--  1 root root    978M Jul 14 20:10 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-14T16:37:05.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 14 22:25 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-14T20:39:06_1st20epochs.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 00:09 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-14T20:39:06_2nd20epochs(transformed_dataset).pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 06:42 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-15T06:25:55_2nd20epochs(transformed_dataset)@.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 08:30 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-15T06:57:06_2nd20epochs(transformed_dataset)@.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 09:27 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-15T08:57:03_2nd20epochs(transformed_dataset)@.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 10:29 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-15T09:29:07_2nd20epochs(transformed_dataset)@.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 16:13 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_mean_loss_epochs40_transformation=[normed]@2023-07-15T12:24:34_epochs=[0, 40].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 19:00 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_mean_loss_epochs40_transformation=[normed]@2023-07-15T12:24:34_epochs=[0, 40]_added_training_loss_sum_epochs=[40, 60]@2023-07-15T18:44:19.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 21:03 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_mean_loss_epochs40_transformation=[normed]@2023-07-15T12:24:34_epochs=[0, 40]_added_training_loss_sum_epochs=[40, 60]@2023-07-15T19:08:24.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 21:51 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_mean_loss_epochs40_transformation=[normed]@2023-07-15T12:24:34_epochs=[0, 40]_added_training_on_val_loss_sum_epochs=[40, 60]@2023-07-15T21:22:43.pt'\n",
      "-rw-r--r--  1 root root    978M Jul 15 18:22 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_mean_loss_epochs40_transformation=[normed]@2023-07-15T16:45:42_epochs=[40, 60].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:20 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=0_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:14 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=10_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:20 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=11_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:25 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=12_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:31 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=13_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:36 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=14_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:42 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=15_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:47 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=16_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:52 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=17_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:57 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=18_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 12:03 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=19_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:25 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=1_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:31 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=2_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:37 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=3_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:42 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=4_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:48 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=5_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:53 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=6_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 10:59 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=7_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:04 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=8_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 11:09 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed; rolling=0.5s; color_noise]@2023-07-16T10:14:41_epoch=9_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:05 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=0_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:00 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=10_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:05 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=11_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:11 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=12_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:16 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=13_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:21 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=14_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:27 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=15_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:32 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=16_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:37 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=17_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:42 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=18_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 14:48 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=19_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:11 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=1_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:16 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=2_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:21 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=3_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:26 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=4_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:32 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=5_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:38 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=6_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:43 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=7_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:48 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=8_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 13:54 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T12:59:29_epoch=9_epochs=[0, 20].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:26 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=0_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:32 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=1_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:37 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=2_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:43 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=3_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:48 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=4_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:54 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=5_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 15:59 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=6_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 16:05 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=7_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 16:10 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=8_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul 16 16:16 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_mean_loss_train+val_stratified_epochs20_transformation=[normed]@2023-07-16T15:20:32_epoch=9_epochs=[0, 10].pt'\n",
      "-rw-r--r--  1 root root    978M Jul  6 18:03  gdsc22_passt_kd_explr_t40_f4_45epochs@2023-07-06T08:17:35.pt\n",
      "-rw-r--r--  1 root root    978M Jul 11 07:17  gdsc22_passt_kd_explr_t40_f4_45epochs_augmented_train@2023-07-10T17:13:39.pt\n",
      "-rw-r--r--  1 root root    978M Jul 12 01:42  gdsc22_passt_kd_explr_t40_f4_45epochs_augmented_train@2023-07-11T09:39:09.pt\n",
      "-rw-r--r--  1 root root    978M Jul  5 22:08  gdsc22_passt_kd_t40_f4@2023-07-05T17:40:28.pt\n",
      "-rw-r--r--  1 root root    978M Jul  6 07:35  gdsc22_passt_swa_explr_t40_f4@2023-07-06T06:53:42.pt\n",
      "-rw-r--r--  1 root root    978M Jul  6 07:54  gdsc22_passt_swa_explr_t40_f4@2023-07-06T07:40:26.pt\n",
      "-rw-r--r--  1 root root    978M Jul  5 13:12  gdsc22_passt_swa_t40_f4.pt\n",
      "-rw-r--r--  1 root root    978M Jul  4 23:23  gdsc22_past_swa_t10_f3.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -ahl /root/data/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b380454-08df-4c84-a704-2f72d7585749",
   "metadata": {},
   "source": [
    "## Finetune model trained on first 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26335784-cded-4d9e-816f-e9559f4d3455",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PaSST pre-trained on AudioSet (with KD) Patch 16 stride 10 structured patchout mAP=486 \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=66, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=66, bias=True)\n",
      ")\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PaSSTGDSCIface(\n",
       "  (mel): AugmentMelSTFT(\n",
       "    winsize=800, hopsize=320\n",
       "    (freqm): FrequencyMasking()\n",
       "    (timem): TimeMasking()\n",
       "  )\n",
       "  (net): PaSST(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (pre_logits): Identity()\n",
       "    (head): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=66, bias=True)\n",
       "    )\n",
       "    (head_dist): Linear(in_features=768, out_features=66, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TORCH_LOAD_FILE = os.path.join(project_dir, 'checkpoints', 'gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_mean_loss_epochs40_transformation=[normed]@2023-07-15T12:24:34_epochs=[0, 40].pt')\n",
    "\n",
    "model = get_pretrained_passt_model(mode=\"all\", n_classes=66, s_patchout_t=40, s_patchout_f=4)\n",
    "state = torch.load(TORCH_LOAD_FILE, map_location=torch.device('cpu'))\n",
    "print(model.load_state_dict(state['model_state_dict']))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6294650d-4a6e-492d-9a62-7921db7cc2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 15 19:08:17 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   50C    P0    29W /  70W |    924MiB / 15109MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      5815      C   /opt/conda/bin/python             921MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e7321d2-0dbf-4dec-8715-8de9358c840d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP2: Additional 20epochs (40-60) using <class 'utils.dataset_augmentations.BuzzMapTransformedDataset'>: Transformations=(<utils.dataset_augmentations.NormalizeAmplitudes object at 0x7ff3ef56acb0>); Mapper=(<class 'utils.gdsc22_dataset_legacy.BuzzMapDataset'>: reading data from /root/data/gdsc_data/data_processed/data_8c86715/05_split_resampled_validation_into_windows/train/metadata.csv. Dataset stats: shape=(2160, 3))\n",
      "\n",
      "Val using <class 'utils.dataset_augmentations.BuzzMapTransformedDataset'>: Transformations=(<utils.dataset_augmentations.NormalizeAmplitudes object at 0x7ff3ef56ac80>); Mapper=(<class 'utils.gdsc22_dataset_legacy.BuzzMapDataset'>: reading data from /root/data/gdsc_data/data_processed/data_8c86715/05_split_resampled_validation_into_windows/val/metadata.csv. Dataset stats: shape=(358, 3))\n",
      "\n",
      "Initial Learning rate:2.5e-06\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:632: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([32, 1, 128, 1000])\n",
      "self.norm(x) torch.Size([32, 768, 12, 99])\n",
      " patch_embed :  torch.Size([32, 768, 12, 99])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 99])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 12, 1])\n",
      "X Before time Patchout of 40  torch.Size([32, 768, 12, 99])\n",
      "X after time Patchout torch.Size([32, 768, 12, 59])\n",
      "X Before Freq Patchout of 4  torch.Size([32, 768, 12, 59])\n",
      " \n",
      " X after freq Patchout:  torch.Size([32, 768, 8, 59])\n",
      "X flattened torch.Size([32, 472, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 2, 768])\n",
      " self.cls_tokens.shape torch.Size([32, 1, 768])\n",
      " self.dist_token.shape torch.Size([32, 1, 768])\n",
      " final sequence x torch.Size([32, 474, 768])\n",
      " after 12 atten blocks x torch.Size([32, 474, 768])\n",
      "forward_features torch.Size([32, 768])\n",
      "head torch.Size([32, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TrainL 8.61: 100%|██████████| 68/68 [01:01<00:00,  1.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=40): 1023.9627685546875\n",
      "F1 score(train) 0.7795568786631606\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 8.10: 100%|██████████| 12/12 [00:14<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 81.45458984375\n",
      "F1 score(val) 0.8493284440503093\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.09: 100%|██████████| 68/68 [01:02<00:00,  1.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=41): 700.392822265625\n",
      "F1 score(train) 0.8354525577255543\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.03: 100%|██████████| 12/12 [00:16<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 73.40998077392578\n",
      "F1 score(val) 0.8482749501091149\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 7.68: 100%|██████████| 68/68 [01:08<00:00,  1.01s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=42): 568.9346313476562\n",
      "F1 score(train) 0.8767540458557235\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.09: 100%|██████████| 12/12 [00:16<00:00,  1.41s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 78.00786590576172\n",
      "F1 score(val) 0.855961440683306\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 4.77: 100%|██████████| 68/68 [01:10<00:00,  1.04s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=43): 550.0682983398438\n",
      "F1 score(train) 0.8703874924334881\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.92: 100%|██████████| 12/12 [00:16<00:00,  1.36s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 76.50442504882812\n",
      "F1 score(val) 0.8694600632728378\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.38: 100%|██████████| 68/68 [01:09<00:00,  1.03s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=44): 530.0186767578125\n",
      "F1 score(train) 0.8847627639109262\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 1.73: 100%|██████████| 12/12 [00:16<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 80.56640625\n",
      "F1 score(val) 0.8440193819498035\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.80: 100%|██████████| 68/68 [01:02<00:00,  1.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=45): 440.1483154296875\n",
      "F1 score(train) 0.9051234129750901\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 1.12: 100%|██████████| 12/12 [00:13<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 73.96685791015625\n",
      "F1 score(val) 0.8891745238963893\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.28: 100%|██████████| 68/68 [00:57<00:00,  1.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=46): 376.23321533203125\n",
      "F1 score(train) 0.9028889611425904\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.37: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 79.79823303222656\n",
      "F1 score(val) 0.8891745238963893\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.33: 100%|██████████| 68/68 [00:58<00:00,  1.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=47): 363.2059326171875\n",
      "F1 score(train) 0.9148660079648661\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 2.25: 100%|██████████| 12/12 [00:13<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 79.3635025024414\n",
      "F1 score(val) 0.868972503694369\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.29: 100%|██████████| 68/68 [01:00<00:00,  1.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=48): 381.99853515625\n",
      "F1 score(train) 0.9046160282442391\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.20: 100%|██████████| 12/12 [00:14<00:00,  1.24s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 72.13119506835938\n",
      "F1 score(val) 0.868972503694369\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 5.81: 100%|██████████| 68/68 [01:05<00:00,  1.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=49): 304.096435546875\n",
      "F1 score(train) 0.9182095197256769\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.53: 100%|██████████| 12/12 [00:16<00:00,  1.38s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 78.20600891113281\n",
      "F1 score(val) 0.873668817481592\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.86: 100%|██████████| 68/68 [01:10<00:00,  1.04s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=50): 341.2222900390625\n",
      "F1 score(train) 0.900424565466959\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.03: 100%|██████████| 12/12 [00:16<00:00,  1.39s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 78.49827575683594\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 2.5e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.91: 100%|██████████| 68/68 [01:01<00:00,  1.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=51): 310.647705078125\n",
      "F1 score(train) 0.9240570422213459\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.03: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 81.09269714355469\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 1.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.32: 100%|██████████| 68/68 [00:57<00:00,  1.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=52): 313.8778381347656\n",
      "F1 score(train) 0.9215260647278528\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.91: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 82.07026672363281\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 1.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.14: 100%|██████████| 68/68 [00:58<00:00,  1.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=53): 278.48541259765625\n",
      "F1 score(train) 0.9445882306051324\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.58: 100%|██████████| 12/12 [00:13<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 79.81758880615234\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 1.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.70: 100%|██████████| 68/68 [00:58<00:00,  1.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=54): 304.69805908203125\n",
      "F1 score(train) 0.9409511697480869\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 1.51: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 76.67391204833984\n",
      "F1 score(val) 0.8721536659664404\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 1.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.67: 100%|██████████| 68/68 [00:57<00:00,  1.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=55): 254.62744140625\n",
      "F1 score(train) 0.9398594494851534\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 1.86: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 77.1438217163086\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 1.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.19: 100%|██████████| 68/68 [00:57<00:00,  1.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=56): 247.6551971435547\n",
      "F1 score(train) 0.9453921895702665\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.31: 100%|██████████| 12/12 [00:13<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 79.68685150146484\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 1.25e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.39: 100%|██████████| 68/68 [00:57<00:00,  1.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=57): 248.55821228027344\n",
      "F1 score(train) 0.9568705407599385\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.09: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 78.74614715576172\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 6.25e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 0.87: 100%|██████████| 68/68 [00:57<00:00,  1.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=58): 259.5141906738281\n",
      "F1 score(train) 0.9391356623845181\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.78: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 78.5841064453125\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 6.25e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/68 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "TrainL 2.53: 100%|██████████| 68/68 [00:58<00:00,  1.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Train Loss(epoch=59): 388.72918701171875\n",
      "F1 score(train) 0.9072773314257395\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n",
      "ValL 0.74: 100%|██████████| 12/12 [00:13<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "\n",
      "Val Loss: 77.09075927734375\n",
      "F1 score(val) 0.8669523016741669\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "New lr: 6.25e-07\n"
     ]
    }
   ],
   "source": [
    "adamw = AdamW(model.net.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "adamw.load_state_dict(state['optimizer_state_dict'])\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "scaler.load_state_dict(state['grad_scaler_state_dict'])\n",
    "\n",
    "# kept default for the model @2023-07-15T16:45:42\n",
    "lr_scheduler = ReduceLROnPlateauHumanized(adamw)\n",
    "lr_scheduler.load_state_dict(state['lr_scheduler_state_dict'])\n",
    "\n",
    "ts2 = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "TORCH_SAVE_FILE, EXT = os.path.splitext(TORCH_LOAD_FILE)\n",
    "\n",
    "\n",
    "train_loop_dataloader = val_dataloader_normed\n",
    "val_loop_dataloader = new_val_dataloader_normed\n",
    "\n",
    "epoch_begin = state['epoch'] + 1\n",
    "epoch_count = 20 \n",
    "epoch_end = epoch_begin + epoch_count\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"STEP2: Additional {epoch_count}epochs ({epoch_begin}-{epoch_end}) using {train_loop_dataloader}\\r\\n\")\n",
    "print(f\"Val using {val_loop_dataloader}\\r\\n\")\n",
    "print(f\"Initial Learning rate:{lr_scheduler.get_last_lr()}\\r\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "LOSS_REDUCTION_METHOD = 'mean'\n",
    "\n",
    "def loss_reduce_fn(loss, method):\n",
    "    _reduce_fn = getattr(loss, method)\n",
    "    return _reduce_fn()\n",
    "\n",
    "\n",
    "for epoch in range(epoch_begin, epoch_begin + epoch_count):\n",
    "    y_tr_pred_f1 = []\n",
    "    y_tr_true_f1 = []\n",
    "    train_loss = []\n",
    "\n",
    "    model.train()\n",
    "    for _, _, _, X_batch, y in (progress := tqdm.tqdm(train_loop_dataloader)):\n",
    "        adamw.zero_grad()        \n",
    "        \n",
    "        X_batch = X_batch.to(device)\n",
    "        y = y.to(device)\n",
    "        X_batch_spectr = model.mel(X_batch).unsqueeze(1)\n",
    "        \n",
    "        with  torch.cuda.amp.autocast(): \n",
    "            y_hat, emb = model.net(X_batch_spectr)     \n",
    "            \n",
    "            loss = F.cross_entropy(y_hat, y, reduction=\"none\")\n",
    "            loss_reduce_method = getattr(loss, LOSS_REDUCTION_METHOD)\n",
    "            loss = loss_reduce_method()\n",
    "            \n",
    "            train_loss.append(loss.detach().cpu())\n",
    "            progress.set_description(f\"TrainL {float(loss):.2f}\")\n",
    "            \n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(adamw)\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        \n",
    "        prob = F.softmax(y_hat.detach(), dim=1)\n",
    "        y_pred = torch.argmax(prob.detach(), dim=1)\n",
    "        y_tr_pred_f1.append(y_pred.cpu())\n",
    "        y_tr_true_f1.append(y.cpu().argmax(dim=1))\n",
    "\n",
    "    f_score_train = f1_score(torch.cat(y_tr_true_f1).numpy(), torch.cat(y_tr_pred_f1).numpy(), average='macro')\n",
    "    total_train_loss = torch.stack(train_loss, dim=0)\n",
    "    total_train_reduction_method = getattr(total_train_loss, LOSS_REDUCTION_METHOD)\n",
    "    total_train_loss = total_train_reduction_method()\n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "    print(f\"Train Loss(epoch={epoch}): {total_train_loss}\")\n",
    "    print(f\"F1 score(train) {f_score_train}\")\n",
    "    print(\"\\r\\n\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "\n",
    "    val_loss = []\n",
    "    y_val_pred_f1 = []\n",
    "    y_val_true_f1 = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, _, _, X_batch, y_val in (progress := tqdm.tqdm(val_loop_dataloader)):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            X_batch_spectr = model.mel(X_batch).unsqueeze(1)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                y_hat, emb = model.net(X_batch_spectr)\n",
    "            \n",
    "                _loss = F.cross_entropy(y_hat, y_val, reduction=\"none\")\n",
    "                _loss_reduce_method = getattr(_loss, LOSS_REDUCTION_METHOD)\n",
    "                _loss = _loss_reduce_method()\n",
    "                val_loss.append(_loss.detach().cpu())\n",
    "            \n",
    "            progress.set_description(f\"ValL {float(_loss):.2f}\")            \n",
    "\n",
    "            prob = F.softmax(y_hat.detach(), dim=1)\n",
    "            y_pred = torch.argmax(prob.detach(), dim=1)\n",
    "\n",
    "            y_val_pred_f1.append(y_pred.cpu())\n",
    "            y_val_true_f1.append(y_val.cpu().argmax(dim=1))\n",
    "            \n",
    "    f1_score_val = f1_score(torch.cat(y_val_true_f1).numpy(), torch.cat(y_val_pred_f1).numpy(), average='macro')\n",
    "    total_val_loss = torch.stack(val_loss, dim=0)\n",
    "    total_val_loss_reduction_method = getattr(total_val_loss, LOSS_REDUCTION_METHOD)\n",
    "    total_val_loss = total_val_loss_reduction_method()\n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "    print(f\"Val Loss: {total_val_loss}\")\n",
    "    print(f\"F1 score(val) {f1_score_val}\")\n",
    "    print(\"\\r\\n\")    \n",
    "    print(f\"=\" * 80)\n",
    "    print(\"\\r\\n\")\n",
    "       \n",
    "    writer.add_scalar('Loss/train', total_train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', total_val_loss, epoch)\n",
    "    writer.add_scalar('F1/train', f_score_train, epoch)\n",
    "    writer.add_scalar('F1/val', f1_score_val, epoch)\n",
    "    writer.add_scalar('LR', lr_scheduler.get_last_lr(), epoch)\n",
    "\n",
    "    lr_scheduler.step(f1_score_val)\n",
    "    print(f\"New lr: {lr_scheduler.get_last_lr()}\")\n",
    "       \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': adamw.state_dict(),\n",
    "            'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "            'grad_scaler_state_dict': scaler.state_dict(),\n",
    "        }, f\"{TORCH_SAVE_FILE}_added_training_on_val_loss_sum_epochs=[{epoch_begin}, {epoch_begin+epoch_count}]@{ts2}{EXT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1235bc5e-2e69-45c2-b76d-a383a519944b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 15 06:16:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P0    26W /  70W |  14936MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76709ea-bd4f-4cfe-8634-1e94749f6ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PaSST pre-trained on AudioSet (with KD) Patch 16 stride 10 structured patchout mAP=486 \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=66, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=66, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/data/checkpoints/gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-15T06:07:22.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_eval \u001b[38;5;241m=\u001b[39m get_pretrained_passt_model(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m66\u001b[39m, s_patchout_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, s_patchout_f\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTORCH_SAVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_eval\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/data/checkpoints/gdsc22_passt_kd_NOEXP_lr_mismatch_size_t40_f4_fixed_shuffle_epochs20_transformation=[normed]_epochs20_transformations=[normed; gain; roll; mixup; color_noise]@2023-07-15T06:07:22.pt'"
     ]
    }
   ],
   "source": [
    "model_eval = get_pretrained_passt_model(mode=\"all\", n_classes=66, s_patchout_t=0, s_patchout_f=0)\n",
    "state = torch.load(TORCH_SAVE_PATH)\n",
    "model_eval.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f108a0e4-4da7-42cc-930e-18d6e022df43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_eval = model_eval.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991608ed-62f6-4f71-a16d-0d2567d1f804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/169 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/functional.py:632: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:801.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "/root/data/exploratory_notebooks/da/src/hear21passt/hear21passt/models/passt.py:292: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([12, 1, 128, 1000])\n",
      "self.norm(x) torch.Size([12, 768, 12, 99])\n",
      " patch_embed :  torch.Size([12, 768, 12, 99])\n",
      " self.time_new_pos_embed.shape torch.Size([1, 768, 1, 99])\n",
      " self.freq_new_pos_embed.shape torch.Size([1, 768, 12, 1])\n",
      "X flattened torch.Size([12, 1188, 768])\n",
      " self.new_pos_embed.shape torch.Size([1, 2, 768])\n",
      " self.cls_tokens.shape torch.Size([12, 1, 768])\n",
      " self.dist_token.shape torch.Size([12, 1, 768])\n",
      " final sequence x torch.Size([12, 1190, 768])\n",
      " after 12 atten blocks x torch.Size([12, 1190, 768])\n",
      "forward_features torch.Size([12, 768])\n",
      "head torch.Size([12, 66])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [03:09<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1069.6846923828125\n",
      "================================================================================\n",
      "F1 score(val) 0.8392231749961314\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    val_loss = 0.0\n",
    "    ypred_f1 = []\n",
    "    ytrue_f1 = []\n",
    "    model_eval.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, y_val in tqdm.tqdm(val_dataloader):\n",
    "            batch = batch.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "\n",
    "            y_hat, emb = model_eval(batch)\n",
    "            _loss = F.cross_entropy(y_hat, y_val, reduction=\"none\")\n",
    "\n",
    "            prob = F.softmax(y_hat, dim=1)\n",
    "            y_pred = torch.argmax(prob, dim=1)\n",
    "\n",
    "            ypred_f1.append(y_pred.cpu())\n",
    "            ytrue_f1.append(y_val.cpu())\n",
    "\n",
    "            val_loss += _loss.sum()\n",
    "\n",
    "    f1_score_val = f1_score(torch.cat(ytrue_f1).numpy(), torch.cat(ypred_f1).numpy(), average='macro')\n",
    "\n",
    "    print(f\"Val Loss: {val_loss}\")\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"F1 score(val) {f1_score_val}\")\n",
    "    print(f\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c65e87d-4d17-432f-bfb1-1f7f6871dcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 14 20:22:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   48C    P0    26W /  70W |  14666MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb9301-730a-4a36-aa3c-918071d3d05e",
   "metadata": {},
   "source": [
    "\n",
    "## Miscellaneous\n",
    "\n",
    "### Glossary\n",
    "\n",
    "SWA - Stochastic weight averaging\n",
    "\n",
    "### Model stored weights\n",
    "```\n",
    "Downloading: \"https://github.com/kkoutini/PaSST/releases/download/v0.0.1-audioset/passt-s-f128-p16-s10-ap.476-swa.pt\" to /root/.cache/torch/hub/checkpoints/passt-s-f128-p16-s10-ap.476-swa.pt\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "GDSC (custom-gdsc/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:992955421961:image-version/custom-gdsc/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:us-east-1:992955421961:studio-lifecycle-config/clean-trash"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
